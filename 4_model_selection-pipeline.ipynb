{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "After doing feature selection we got our final training data. \n",
    "\n",
    "On this part we are going to try fitting a machine learning model. \n",
    "We are going to consider a decision tree and a random forest.\n",
    "\n",
    "First of all we are going to tune up a decision tree for simplicity. And we are going to train a random forest for performance. \n",
    "It may be the case that the random forest could not improve the performance of the decision tree. Thus we are doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('transformed_train2.csv')\n",
    "test_df = pd.read_csv('transformed_test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have not used the test data. We are going to use it now for evaluating our models. For tunning we are going to use cross validation with 5 folds (to control overfitting) since we do not have that many samples. If we had more, it would be really costly computationally.\n",
    "\n",
    "We are going to use sklearn's gridsearch with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data, model, param_grid, verbose=0):\n",
    "    grid = GridSearchCV(model, param_grid, cv=5, n_jobs=4, verbose=verbose)\n",
    "    grid.fit(data.drop(['Survived'], axis=1), data['Survived'])\n",
    "    \n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    print(\"Best score: \", grid.best_score_)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n",
    "\n",
    "In the case of the decision tree, [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) let's us play with:\n",
    "\n",
    "* criterion{“gini”, “entropy”}: The function to measure the quality of a split.\n",
    "* splitter{\"best\", \"random\"}: Chose a feature and get the best possible split or get the best split of a random set of splits (less probable to overfit).\n",
    "* max_depth{int}: Controls the maximun depth\n",
    "* min_samples_split{int}: Minimun namber of samples needed to do an split.\n",
    "* min_samples_leaf{int}: Minimun number of samples needed to be a leaf.\n",
    "* min_weight_fraction_leaf{float}: In this case (assuming all the data has the same weight) it is the proportion of samples needed in order to be a leaf.\n",
    "* random_state{int}: Sets a random seed.\n",
    "* max_leaf_nodes{int}: Sets an upperbond to the size of the tree in leafs\n",
    "* min_impurity_decrease{float}: There will be an split only if the decrease of inpurity is greater than this threshold\n",
    "* ccp_alpha{float}: Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen\n",
    "\n",
    "Trying to perform a grid seach with all these many parameters is insane. \n",
    "\n",
    "From [towardsdatascience](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680) we found out that:\n",
    "* max_depth, max_leaf_nodes, min_samples_split, and min_samples_leaf are all stopping criteria\n",
    "* min_weight_fraction_leaf and min_impurity_decrease are pruning methods.\n",
    "\n",
    "We will consider only max_depth and min_impurity decrease.\n",
    "\n",
    "\n",
    "At the end we want to tune up:\n",
    "* criterion\n",
    "* splitter\n",
    "* max_depth\n",
    "* min_impurity_decrease\n",
    "* ccp_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 5, 'min_impurity_decrease': 0.0, 'splitter': 'best'}\n",
      "Best score:  0.825844577957254\n"
     ]
    }
   ],
   "source": [
    "# First grid\n",
    "parm_grid = {\n",
    "    'max_depth': [None, 5, 10, 20, 50, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid = grid_search(train_df, tree, parm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'ccp_alpha': 0.0, 'max_depth': 5, 'min_impurity_decrease': 0.0}\n",
      "Best score:  0.825844577957254\n"
     ]
    }
   ],
   "source": [
    "# Second grid\n",
    "parm_grid = {\n",
    "    'max_depth': range(1, 10),\n",
    "    'ccp_alpha': np.arange(0.0, 0.1, 0.01),\n",
    "    'min_impurity_decrease': np.arange(0.0, 1.0, 0.01),\n",
    "}\n",
    "grid = grid_search(train_df, tree, parm_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our best tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tree = grid.best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "In this case we have the same parameters that in the decision tree. But in addition we have:\n",
    "* n_estimators{int}: The number of trees we want\n",
    "\n",
    "There are a bit more, but we are not going to take them into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 5, 'min_impurity_decrease': 0.0, 'n_estimators': 30}\n",
      "Best score:  0.8300403821530582\n"
     ]
    }
   ],
   "source": [
    "# First grid\n",
    "parm_grid = {\n",
    "    'n_estimators': [10, 20, 30, 40, 50],\n",
    "    'max_depth': [None, 5, 10, 20, 50, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "}\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "grid = grid_search(train_df, forest, parm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 4, 'min_impurity_decrease': 0.0, 'n_estimators': 32}\n",
      "Best score:  0.8328572835615089\n"
     ]
    }
   ],
   "source": [
    "# Second grid\n",
    "parm_grid = {\n",
    "    'n_estimators': range(25, 35),\n",
    "    'max_depth': range(2, 10),\n",
    "    'criterion': ['entropy'],\n",
    "    'min_impurity_decrease': np.arange(0.0, .1, 0.05),\n",
    "    'ccp_alpha': np.arange(0.0, .1, 0.05),\n",
    "}\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "grid = grid_search(train_df, forest, parm_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we got our best random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision\n",
    "\n",
    "Now let's compare the performance of both models on the actual test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree accuracy: 0.8100558659217877\n",
      "RF accuracy 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "print(\"Tree accuracy:\", best_tree.score(test_df.drop(['Survived'], axis=1), test_df['Survived']))\n",
    "print(\"RF accuracy\", best_forest.score(test_df.drop(['Survived'], axis=1), test_df['Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end we are not that happy with the results. This accuracy is far less than we spected. I will probably try to use a neural net. But for now let's do de pipeline and submite my predictions to kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Once we got our best model, we are going to create a pipeline in which will have as input the raw original dataset and as output the prediction. In order to do this we are going to use sklearn's pipeline. In order to include all our data transformations in the pipeline we need to create some classes which are compatible with sklern's pipeline environment.\n",
    "\n",
    "In order to make this classes compatible we need to create the fit and the transform methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a class which implements data inputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "class DataFrameImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.catImputer = SimpleImputer(strategy='most_frequent')\n",
    "        self.numImputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # Converting to np.nan all the missing values\n",
    "        X.applymap(lambda x: x if x else np.nan)\n",
    "\n",
    "        # Creating a list with the numerical features\n",
    "        self.num = [X.dtypes.index[i] for i, dtype in enumerate(X.dtypes) if dtype != object]\n",
    "\n",
    "\n",
    "        # Creating a list with all the categorical features\n",
    "        self.cat = [X.dtypes.index[i] for i, dtype in enumerate(X.dtypes) if dtype == object]\n",
    "\n",
    "        self.catImputer.fit(X[self.cat])\n",
    "        self.numImputer.fit(X[self.num])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[self.cat] = self.catImputer.transform(X[self.cat])\n",
    "        X[self.num] = self.numImputer.transform(X[self.num])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we are going to create a class which can select a subset of features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we need a helper class which allows us to merge the data sets of the Encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class DataFrameEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categories = [\"Sex\", \"Pclass\", \"Embarked\"]):\n",
    "        self.encoder = OneHotEncoder(drop=\"if_binary\",sparse=False)\n",
    "        self.categories = categories\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X[self.categories])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        encoded = pd.DataFrame(\n",
    "            self.encoder.transform(X[self.categories]),\n",
    "            columns=self.encoder.get_feature_names_out()\n",
    "            )\n",
    "\n",
    "        encoded.index = X.index\n",
    "        return X.join(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "selector_1 = DataFrameSelector(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "imputer = DataFrameImputer()\n",
    "encoder = DataFrameEncoder()\n",
    "selector_2 = DataFrameSelector(['Age', 'SibSp', 'Sex_male', 'Pclass_1.0', 'Pclass_3.0','Embarked_C', 'Embarked_Q'])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "pipeline = Pipeline([(\"selector_1\", selector_1), (\"imputer\", imputer), (\"encoder\", encoder), (\"selector_2\", selector_2), (\"scaler\", scaler), (\"forest\", best_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2171/3340383623.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat] = self.catImputer.transform(X[self.cat])\n",
      "/tmp/ipykernel_2171/3340383623.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num] = self.numImputer.transform(X[self.num])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector_1',\n",
       "                 DataFrameSelector(attribute_names=['Pclass', 'Sex', 'Age',\n",
       "                                                    'SibSp', 'Parch', 'Fare',\n",
       "                                                    'Embarked'])),\n",
       "                ('imputer', DataFrameImputer()),\n",
       "                ('encoder', DataFrameEncoder()),\n",
       "                ('selector_2',\n",
       "                 DataFrameSelector(attribute_names=['Age', 'SibSp', 'Sex_male',\n",
       "                                                    'Pclass_1.0', 'Pclass_3.0',\n",
       "                                                    'Embarked_C',\n",
       "                                                    'Embarked_Q'])),\n",
       "                ('scaler', MinMaxScaler()),\n",
       "                ('forest',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=4,\n",
       "                                        n_estimators=32))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "pipeline.fit(data.drop(['Survived'], axis=1), data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Finally we end up on this step. We are going to predict the test set of kaggle and we are going to store them along with the ids in prediction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2171/3340383623.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat] = self.catImputer.transform(X[self.cat])\n",
      "/tmp/ipykernel_2171/3340383623.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num] = self.numImputer.transform(X[self.num])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "prediction = pipeline.predict(test)\n",
    "ids = test['PassengerId']\n",
    "\n",
    "prediction_df = pd.DataFrame(data=zip(ids, prediction), columns=['PassengerId', 'Survived'], index=None)\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 0.77990 which positionates ourselves on the 3254 place at the moment of doing this notebook. It is a quite frustrating result being honest.\n",
    "We will try to employ a neural network to try to beat this result."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285e3f512f08f9ef64bd5423181a4a96029698380e996ba4dc1551887e492013"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
